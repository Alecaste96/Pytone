{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo le librerie necessarie\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "f1 = \"portieri_19.xlsx\"\n",
    "f2 = \"portieri_20.xlsx\"\n",
    "f3 = \"portieri_21.xlsx\"\n",
    "f4 = \"portieri_22.xlsx\"\n",
    "f5 = \"portieri_23.xlsx\"\n",
    "\n",
    "# Elimino le colonne non utili per la nostra analisi\n",
    "colonne_da_eliminare = ['Player', 'Nation', 'Pos', 'Squad', 'Age', 'MP', 'Starts', 'Min', 'Save%',\n",
    "                        'CS%', 'PKatt', 'PKA', 'PKm', 'Save%.1', 'FK', 'CK', 'OG', 'Cmp', 'Att', \n",
    "                        'Cmp%', 'Att (GK)', 'Thr', 'Launch%', 'AvgLen', 'Att.1', 'Launch%.1', \n",
    "                        'AvgLen.1', 'Opp', 'Stp%']\n",
    "\n",
    "# Unisco i file Excel\n",
    "df1 = pd.read_excel(f1)\n",
    "df2 = pd.read_excel(f2).iloc[1:].reset_index(drop=True)\n",
    "df3 = pd.read_excel(f3).iloc[1:].reset_index(drop=True)\n",
    "df4 = pd.read_excel(f4).iloc[1:].reset_index(drop=True)\n",
    "df_unito = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Rimuovo le colonne inutili\n",
    "df_unito = df_unito.drop(columns=colonne_da_eliminare, errors='ignore')\n",
    "\n",
    "df5 = pd.read_excel(f5).iloc[1:].reset_index(drop=True)\n",
    "df5 = df5.drop(columns=colonne_da_eliminare, errors='ignore')\n",
    "\n",
    "# Rimuovo le righe con NaN nel dataset unito\n",
    "df_unito = df_unito.dropna()\n",
    "\n",
    "# Separare le colonne in input (X) e output (y)\n",
    "X = df_unito.drop(columns = 'Saves', errors = 'ignore')\n",
    "y = df_unito['Saves']\n",
    "\n",
    "# Normalizzazione degli input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Divisione in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Costruzione della rete neurale\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Modificato il livello di output\n",
    "])\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Addestramento del modello\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Pre-processamento dei nuovi dati (df5)\n",
    "df_new = df5.dropna()  # Rimuovo le righe con NaN nei nuovi dati\n",
    "\n",
    "# Verifica che le colonne siano allineate con il DataFrame di addestramento\n",
    "X_new = df_new[X.columns]\n",
    "\n",
    "# Pre-elabora i dati nuovi\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Usa il modello per fare previsioni\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "\n",
    "# Sostituisci i valori negativi con 0\n",
    "y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "# Visualizza le previsioni\n",
    "df_new['Pred_Saves'] = y_pred[:, 0]\n",
    "\n",
    "print(df_new[['Saves', 'Pred_Saves']])\n",
    "\n",
    "# Calcola separatamente la somma delle previsioni per le parate\n",
    "somma_saves_attesi = df_new['Pred_Saves'].sum()\n",
    "\n",
    "# Calcola separatamente la somma effettiva delle parate nei dati originali\n",
    "somma_saves_effettivi = df_new['Saves'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola separatamente la somma delle previsioni per Gls e Ast\n",
    "somma_goal_attesi = df_new['Pred_Saves'].sum()\n",
    "\n",
    "# Calcola separatamente la somma effettiva dei goal e assist nei dati originali\n",
    "somma_goal = df_new['Saves'].sum()\n",
    "\n",
    "# Risultati dei gol\n",
    "print(f\"La somma delle parate è {somma_saves_effettivi}\")\n",
    "print(f\"La somma dei goal ottenuti dalla rete neurale è {somma_saves_attesi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errore\n",
    "\n",
    "diff_saves = abs(somma_saves_effettivi - somma_saves_attesi)\n",
    "\n",
    "# Trovo l'errore medio dividendo per il numero di giocatori \n",
    "\n",
    "avg_err_saves = diff_saves/194  \n",
    "\n",
    "print(f\"La differenza fra la somma effettiva e quella della rete neurale è di {diff_saves} parate\")\n",
    "print(f\"L'errore medio per le parate è {avg_err_saves} per giocatore\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
