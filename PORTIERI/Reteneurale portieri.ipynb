{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo le librerie necessarie\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = \"portieri_19.xlsx\"\n",
    "f2 = \"portieri_20.xlsx\"\n",
    "f3 = \"portieri_21.xlsx\"\n",
    "f4 = \"portieri_22.xlsx\"\n",
    "f5 = \"portieri_23.xlsx\"\n",
    "\n",
    "# Elimino le colonne che abbiamo scoperto essere non utili per la nostra analisi\n",
    "\n",
    "colonne_da_eliminare = [['Player', 'Nation', 'Pos', 'Squad','Age', 'MP', 'Starts', 'Min', 'GA', 'SoTA', 'Saves', 'Save%', 'CS',\n",
    "                          'CS%', 'PKatt', 'PKA', 'PKsv', 'PKm', 'Save%.1', 'FK', 'CK', 'OG', 'Cmp', 'Att', 'Cmp%', 'Att (GK)', \n",
    "                          'Thr', 'Launch%', 'AvgLen', 'Att.1', 'Launch%.1', 'AvgLen.1', 'Opp', 'Stp', 'Stp%']]\n",
    "\n",
    "# Unisco i file Excel\n",
    "df1 = pd.read_excel(f1)\n",
    "df2 = pd.read_excel(f2).iloc[1:].reset_index(drop=True)\n",
    "df3 = pd.read_excel(f3).iloc[1:].reset_index(drop=True)\n",
    "df4 = pd.read_excel(f4).iloc[1:].reset_index(drop=True)\n",
    "df_unito = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Rimuovo le colonne\n",
    "df_unito = df_unito.drop(columns=colonne_da_eliminare, errors='ignore')\n",
    "\n",
    "df5 = pd.read_excel(f5).iloc[1:].reset_index(drop=True)\n",
    "df5 = df5.drop(columns=colonne_da_eliminare, errors='ignore')\n",
    "\n",
    "# Separare le colonne in input e output\n",
    "X = df_unito.iloc[:, 2:]  # Tutte le colonne eccetto le prime due\n",
    "y = df_unito.iloc[:, :2]  # Le prime due colonne\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Divisione in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Costruzione della rete neurale\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                             tf.keras.layers.Dense(64, activation='relu'),\n",
    "                             tf.keras.layers.Dense(y_train.shape[1])])\n",
    "\n",
    "# Compilazione\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Addestramento\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Carica il nuovo file e pre-processa i dati\n",
    "df_new = df5  \n",
    "\n",
    "# Verifica che le colonne siano allineate con il DataFrame di addestramento\n",
    "X_new = df_new[X.columns]\n",
    "\n",
    "# Pre-elabora i dati nuovi\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Usa il modello per fare previsioni\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "\n",
    "# Imposto pari a zero i valori negativi in quanto Ã¨ impossibile realizzare un numero negativo di gol\n",
    " \n",
    "y_pred = np.clip(y_pred, 0, None)  # Sostituisci i valori negativi con 0\n",
    "\n",
    "# Visualizza le previsioni\n",
    "df_new['Pred_Gls'] = y_pred[:, 0]  # Previsioni per la prima colonna (Gls)\n",
    "df_new['Pred_Ast'] = y_pred[:, 1]  # Previsioni per la seconda colonna (Ast)\n",
    "\n",
    "print(df_new[['Gls', 'Ast', 'Pred_Gls', 'Pred_Ast']])\n",
    "\n",
    "# Calcola separatamente la somma delle previsioni per Gls e Ast\n",
    "somma_goal_attesi = df_new['Pred_Gls'].sum()\n",
    "somma_assist_attesi = df_new['Pred_Ast'].sum()\n",
    "\n",
    "# Calcola separatamente la somma effettiva dei goal e assist nei dati originali\n",
    "somma_goal = df_new['Gls'].sum()\n",
    "somma_assist = df_new['Ast'].sum()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
